# Roberta

Category: Transformer

Summary

Optimized BERT training with larger batches and longer training.

Key Ideas

- Architecture motivation
- Core building blocks
- Training considerations
- Typical applications

Detailed Flow

```
[Tokens] -> [Embeddings + Positional] -> [Transformer Encoder]*N (optimized training) -> [CLS Pool] -> [Task Head]
```

Canonical Papers

Further Reading

- Search for more resources on Roberta.

