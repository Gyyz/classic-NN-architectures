# Xlnet

Category: Transformer

Summary

Autoregressive pretraining with permutation language modeling.

Key Ideas

- Architecture motivation
- Core building blocks
- Training considerations
- Typical applications

Detailed Flow

```
[Tokens] -> [Permutation LM + relative positions] -> [Transformer-XL backbone] -> [Task head]
```

Canonical Papers

Further Reading

- Search for more resources on Xlnet.

