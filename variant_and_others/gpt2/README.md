# Gpt2

Category: Transformer

Summary

Scaled GPT with WebText training.

Key Ideas

- Architecture motivation
- Core building blocks
- Training considerations
- Typical applications

Minimal Diagram

```
[Input] -> [Layers/Blocks] -> [Output]
```

Canonical Papers
- Language Models are Unsupervised Multitask Learners (Radford et al., 2019) [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf]

Further Reading

- Search for more resources on Gpt2.

Generated on 2025-11-24.